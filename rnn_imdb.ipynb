{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Define the Fields for processing the dataset\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenizer, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Load the IMDb dataset\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, _ = train_data.split(split_ratio=0.01, random_state=random.seed(SEED))\n",
    "\n",
    "\n",
    "# Split the training data to create a validation set\n",
    "# train_data, valid_data = train_data.split(random_state = torch.manual_seed(SEED))\n",
    "train_data, valid_data = train_data.split(split_ratio=0.7)\n",
    "\n",
    "# Build the vocabulary and load pre-trained word embeddings (GloVe)\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Create iterators for the data\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        # Concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e154b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 5\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "# Load the pre-trained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Zero the initial weights of the unknown and padding tokens\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34dd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96732b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8\"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17953756",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0.0m 1.3118689060211182s\n",
      "\tTrain Loss: 0.696 | Train Acc: 47.04%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 42.40%\n",
      "Epoch: 02 | Epoch Time: 0.0m 1.2832648754119873s\n",
      "\tTrain Loss: 0.700 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 42.40%\n",
      "Epoch: 03 | Epoch Time: 0.0m 1.293835163116455s\n",
      "\tTrain Loss: 0.689 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 42.40%\n",
      "Epoch: 04 | Epoch Time: 0.0m 1.2983603477478027s\n",
      "\tTrain Loss: 0.694 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 42.40%\n",
      "Epoch: 05 | Epoch Time: 0.0m 1.3104240894317627s\n",
      "\tTrain Loss: 0.692 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 42.40%\n",
      "Epoch: 06 | Epoch Time: 0.0m 1.3100824356079102s\n",
      "\tTrain Loss: 0.690 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 42.40%\n",
      "Epoch: 07 | Epoch Time: 0.0m 1.2928311824798584s\n",
      "\tTrain Loss: 0.690 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 42.40%\n",
      "Epoch: 08 | Epoch Time: 0.0m 1.2923288345336914s\n",
      "\tTrain Loss: 0.686 | Train Acc: 53.48%\n",
      "\t Val. Loss: 0.714 |  Val. Acc: 38.64%\n",
      "Epoch: 09 | Epoch Time: 0.0m 1.3034937381744385s\n",
      "\tTrain Loss: 0.652 | Train Acc: 62.09%\n",
      "\t Val. Loss: 0.892 |  Val. Acc: 46.95%\n",
      "Epoch: 10 | Epoch Time: 0.0m 1.3108701705932617s\n",
      "\tTrain Loss: 0.626 | Train Acc: 64.41%\n",
      "\t Val. Loss: 0.847 |  Val. Acc: 50.21%\n",
      "Epoch: 11 | Epoch Time: 0.0m 1.2868719100952148s\n",
      "\tTrain Loss: 0.657 | Train Acc: 68.76%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 45.53%\n",
      "Epoch: 12 | Epoch Time: 0.0m 1.2922637462615967s\n",
      "\tTrain Loss: 0.670 | Train Acc: 55.98%\n",
      "\t Val. Loss: 0.765 |  Val. Acc: 42.40%\n",
      "Epoch: 13 | Epoch Time: 0.0m 1.2998056411743164s\n",
      "\tTrain Loss: 0.681 | Train Acc: 54.71%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 52.41%\n",
      "Epoch: 14 | Epoch Time: 0.0m 1.294687271118164s\n",
      "\tTrain Loss: 0.670 | Train Acc: 58.26%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 47.09%\n",
      "Epoch: 15 | Epoch Time: 0.0m 1.284606695175171s\n",
      "\tTrain Loss: 0.627 | Train Acc: 67.15%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 48.51%\n",
      "Epoch: 16 | Epoch Time: 0.0m 1.292551040649414s\n",
      "\tTrain Loss: 0.587 | Train Acc: 68.57%\n",
      "\t Val. Loss: 0.865 |  Val. Acc: 43.32%\n",
      "Epoch: 17 | Epoch Time: 0.0m 1.2906830310821533s\n",
      "\tTrain Loss: 0.521 | Train Acc: 78.51%\n",
      "\t Val. Loss: 0.989 |  Val. Acc: 53.20%\n",
      "Epoch: 18 | Epoch Time: 0.0m 1.2584781646728516s\n",
      "\tTrain Loss: 0.445 | Train Acc: 80.78%\n",
      "\t Val. Loss: 1.142 |  Val. Acc: 41.76%\n",
      "Epoch: 19 | Epoch Time: 0.0m 1.2555630207061768s\n",
      "\tTrain Loss: 0.469 | Train Acc: 75.06%\n",
      "\t Val. Loss: 1.252 |  Val. Acc: 44.89%\n",
      "Epoch: 20 | Epoch Time: 0.0m 1.2713954448699951s\n",
      "\tTrain Loss: 0.436 | Train Acc: 79.22%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 43.32%\n",
      "Epoch: 21 | Epoch Time: 0.0m 1.2710511684417725s\n",
      "\tTrain Loss: 0.409 | Train Acc: 83.39%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 57.10%\n",
      "Epoch: 22 | Epoch Time: 0.0m 1.2574713230133057s\n",
      "\tTrain Loss: 0.352 | Train Acc: 86.23%\n",
      "\t Val. Loss: 1.072 |  Val. Acc: 47.23%\n",
      "Epoch: 23 | Epoch Time: 0.0m 1.2568235397338867s\n",
      "\tTrain Loss: 0.300 | Train Acc: 87.98%\n",
      "\t Val. Loss: 1.362 |  Val. Acc: 45.67%\n",
      "Epoch: 24 | Epoch Time: 0.0m 1.263561725616455s\n",
      "\tTrain Loss: 0.274 | Train Acc: 88.64%\n",
      "\t Val. Loss: 1.528 |  Val. Acc: 48.01%\n",
      "Epoch: 25 | Epoch Time: 0.0m 1.2642338275909424s\n",
      "\tTrain Loss: 0.224 | Train Acc: 91.10%\n",
      "\t Val. Loss: 1.374 |  Val. Acc: 47.87%\n",
      "Epoch: 26 | Epoch Time: 0.0m 1.2548770904541016s\n",
      "\tTrain Loss: 0.234 | Train Acc: 92.00%\n",
      "\t Val. Loss: 1.532 |  Val. Acc: 52.56%\n",
      "Epoch: 27 | Epoch Time: 0.0m 1.2551956176757812s\n",
      "\tTrain Loss: 0.206 | Train Acc: 92.00%\n",
      "\t Val. Loss: 1.567 |  Val. Acc: 57.10%\n",
      "Epoch: 28 | Epoch Time: 0.0m 1.2629599571228027s\n",
      "\tTrain Loss: 0.188 | Train Acc: 89.92%\n",
      "\t Val. Loss: 1.465 |  Val. Acc: 57.10%\n",
      "Epoch: 29 | Epoch Time: 0.0m 1.257556438446045s\n",
      "\tTrain Loss: 0.184 | Train Acc: 94.08%\n",
      "\t Val. Loss: 1.543 |  Val. Acc: 50.99%\n",
      "Epoch: 30 | Epoch Time: 0.0m 1.2576687335968018s\n",
      "\tTrain Loss: 0.148 | Train Acc: 93.56%\n",
      "\t Val. Loss: 1.629 |  Val. Acc: 42.68%\n",
      "Epoch: 31 | Epoch Time: 0.0m 1.272157907485962s\n",
      "\tTrain Loss: 0.161 | Train Acc: 95.12%\n",
      "\t Val. Loss: 1.927 |  Val. Acc: 49.43%\n",
      "Epoch: 32 | Epoch Time: 0.0m 1.2598719596862793s\n",
      "\tTrain Loss: 0.161 | Train Acc: 92.48%\n",
      "\t Val. Loss: 2.100 |  Val. Acc: 44.11%\n",
      "Epoch: 33 | Epoch Time: 0.0m 1.2574150562286377s\n",
      "\tTrain Loss: 0.195 | Train Acc: 89.73%\n",
      "\t Val. Loss: 1.713 |  Val. Acc: 44.89%\n",
      "Epoch: 34 | Epoch Time: 0.0m 1.2636091709136963s\n",
      "\tTrain Loss: 0.122 | Train Acc: 96.17%\n",
      "\t Val. Loss: 1.825 |  Val. Acc: 43.47%\n",
      "Epoch: 35 | Epoch Time: 0.0m 1.2634634971618652s\n",
      "\tTrain Loss: 0.138 | Train Acc: 94.04%\n",
      "\t Val. Loss: 1.539 |  Val. Acc: 50.99%\n",
      "Epoch: 36 | Epoch Time: 0.0m 1.2682008743286133s\n",
      "\tTrain Loss: 0.114 | Train Acc: 93.71%\n",
      "\t Val. Loss: 1.958 |  Val. Acc: 51.78%\n",
      "Epoch: 37 | Epoch Time: 0.0m 1.2551918029785156s\n",
      "\tTrain Loss: 0.064 | Train Acc: 96.69%\n",
      "\t Val. Loss: 2.102 |  Val. Acc: 51.78%\n",
      "Epoch: 38 | Epoch Time: 0.0m 1.2518775463104248s\n",
      "\tTrain Loss: 0.157 | Train Acc: 95.46%\n",
      "\t Val. Loss: 2.089 |  Val. Acc: 44.11%\n",
      "Epoch: 39 | Epoch Time: 0.0m 1.2596254348754883s\n",
      "\tTrain Loss: 0.169 | Train Acc: 94.94%\n",
      "\t Val. Loss: 2.287 |  Val. Acc: 51.78%\n",
      "Epoch: 40 | Epoch Time: 0.0m 1.2602927684783936s\n",
      "\tTrain Loss: 0.124 | Train Acc: 95.60%\n",
      "\t Val. Loss: 1.908 |  Val. Acc: 53.34%\n",
      "Epoch: 41 | Epoch Time: 0.0m 1.253664493560791s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.21%\n",
      "\t Val. Loss: 1.552 |  Val. Acc: 52.56%\n",
      "Epoch: 42 | Epoch Time: 0.0m 1.2553093433380127s\n",
      "\tTrain Loss: 0.119 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.587 |  Val. Acc: 50.99%\n",
      "Epoch: 43 | Epoch Time: 0.0m 1.2638278007507324s\n",
      "\tTrain Loss: 0.098 | Train Acc: 95.64%\n",
      "\t Val. Loss: 1.910 |  Val. Acc: 55.54%\n",
      "Epoch: 44 | Epoch Time: 0.0m 1.259504795074463s\n",
      "\tTrain Loss: 0.057 | Train Acc: 97.92%\n",
      "\t Val. Loss: 2.442 |  Val. Acc: 49.43%\n",
      "Epoch: 45 | Epoch Time: 0.0m 1.2527978420257568s\n",
      "\tTrain Loss: 0.043 | Train Acc: 97.92%\n",
      "\t Val. Loss: 1.945 |  Val. Acc: 56.32%\n",
      "Epoch: 46 | Epoch Time: 0.0m 1.2547271251678467s\n",
      "\tTrain Loss: 0.037 | Train Acc: 99.48%\n",
      "\t Val. Loss: 1.703 |  Val. Acc: 44.11%\n",
      "Epoch: 47 | Epoch Time: 0.0m 1.2608075141906738s\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.44%\n",
      "\t Val. Loss: 1.709 |  Val. Acc: 46.45%\n",
      "Epoch: 48 | Epoch Time: 0.0m 1.257599115371704s\n",
      "\tTrain Loss: 0.063 | Train Acc: 98.44%\n",
      "\t Val. Loss: 1.792 |  Val. Acc: 46.45%\n",
      "Epoch: 49 | Epoch Time: 0.0m 1.2543292045593262s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.54%\n",
      "\t Val. Loss: 1.873 |  Val. Acc: 50.21%\n",
      "Epoch: 50 | Epoch Time: 0.0m 1.261289358139038s\n",
      "\tTrain Loss: 0.075 | Train Acc: 98.44%\n",
      "\t Val. Loss: 1.526 |  Val. Acc: 56.32%\n",
      "Epoch: 51 | Epoch Time: 0.0m 1.256962776184082s\n",
      "\tTrain Loss: 0.081 | Train Acc: 97.40%\n",
      "\t Val. Loss: 1.536 |  Val. Acc: 48.65%\n",
      "Epoch: 52 | Epoch Time: 0.0m 1.2628793716430664s\n",
      "\tTrain Loss: 0.059 | Train Acc: 97.73%\n",
      "\t Val. Loss: 2.059 |  Val. Acc: 56.32%\n",
      "Epoch: 53 | Epoch Time: 0.0m 1.2515928745269775s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.662 |  Val. Acc: 48.65%\n",
      "Epoch: 54 | Epoch Time: 0.0m 1.256427526473999s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.54%\n",
      "\t Val. Loss: 1.934 |  Val. Acc: 50.99%\n",
      "Epoch: 55 | Epoch Time: 0.0m 1.260526180267334s\n",
      "\tTrain Loss: 0.053 | Train Acc: 98.44%\n",
      "\t Val. Loss: 2.382 |  Val. Acc: 55.54%\n",
      "Epoch: 56 | Epoch Time: 0.0m 1.2598457336425781s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.968 |  Val. Acc: 57.74%\n",
      "Epoch: 57 | Epoch Time: 0.0m 1.2532646656036377s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.44%\n",
      "\t Val. Loss: 2.770 |  Val. Acc: 50.99%\n",
      "Epoch: 58 | Epoch Time: 0.0m 1.2554230690002441s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.77%\n",
      "\t Val. Loss: 2.848 |  Val. Acc: 50.21%\n",
      "Epoch: 59 | Epoch Time: 0.0m 1.256335735321045s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.21%\n",
      "\t Val. Loss: 2.678 |  Val. Acc: 50.99%\n",
      "Epoch: 60 | Epoch Time: 0.0m 1.2546055316925049s\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.619 |  Val. Acc: 45.67%\n",
      "Epoch: 61 | Epoch Time: 0.0m 1.2583835124969482s\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.441 |  Val. Acc: 44.89%\n",
      "Epoch: 62 | Epoch Time: 0.0m 1.257591724395752s\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.198 |  Val. Acc: 49.43%\n",
      "Epoch: 63 | Epoch Time: 0.0m 1.2556536197662354s\n",
      "\tTrain Loss: 0.013 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.152 |  Val. Acc: 49.43%\n",
      "Epoch: 64 | Epoch Time: 0.0m 1.255467176437378s\n",
      "\tTrain Loss: 0.009 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.206 |  Val. Acc: 48.65%\n",
      "Epoch: 65 | Epoch Time: 0.0m 1.2522544860839844s\n",
      "\tTrain Loss: 0.010 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.346 |  Val. Acc: 47.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Epoch Time: 0.0m 1.2571191787719727s\n",
      "\tTrain Loss: 0.007 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.510 |  Val. Acc: 50.99%\n",
      "Epoch: 67 | Epoch Time: 0.0m 1.262887954711914s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.960 |  Val. Acc: 51.63%\n",
      "Epoch: 68 | Epoch Time: 0.0m 1.2530179023742676s\n",
      "\tTrain Loss: 0.028 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.587 |  Val. Acc: 45.67%\n",
      "Epoch: 69 | Epoch Time: 0.0m 1.2600862979888916s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.466 |  Val. Acc: 48.65%\n",
      "Epoch: 70 | Epoch Time: 0.0m 1.2603058815002441s\n",
      "\tTrain Loss: 0.004 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.419 |  Val. Acc: 49.43%\n",
      "Epoch: 71 | Epoch Time: 0.0m 1.2560935020446777s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.337 |  Val. Acc: 47.23%\n",
      "Epoch: 72 | Epoch Time: 0.0m 1.2604327201843262s\n",
      "\tTrain Loss: 0.024 | Train Acc: 98.44%\n",
      "\t Val. Loss: 2.395 |  Val. Acc: 44.11%\n",
      "Epoch: 73 | Epoch Time: 0.0m 1.2577569484710693s\n",
      "\tTrain Loss: 0.024 | Train Acc: 98.44%\n",
      "\t Val. Loss: 2.600 |  Val. Acc: 47.23%\n",
      "Epoch: 74 | Epoch Time: 0.0m 1.256819725036621s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.731 |  Val. Acc: 51.78%\n",
      "Epoch: 75 | Epoch Time: 0.0m 1.2586991786956787s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.964 |  Val. Acc: 55.54%\n",
      "Epoch: 76 | Epoch Time: 0.0m 1.2552711963653564s\n",
      "\tTrain Loss: 0.025 | Train Acc: 98.77%\n",
      "\t Val. Loss: 2.488 |  Val. Acc: 45.67%\n",
      "Epoch: 77 | Epoch Time: 0.0m 1.2642185688018799s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.634 |  Val. Acc: 53.20%\n",
      "Epoch: 78 | Epoch Time: 0.0m 1.2602753639221191s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.708 |  Val. Acc: 49.43%\n",
      "Epoch: 79 | Epoch Time: 0.0m 1.2556540966033936s\n",
      "\tTrain Loss: 0.019 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.882 |  Val. Acc: 48.79%\n",
      "Epoch: 80 | Epoch Time: 0.0m 1.2583820819854736s\n",
      "\tTrain Loss: 0.071 | Train Acc: 98.25%\n",
      "\t Val. Loss: 2.512 |  Val. Acc: 47.23%\n",
      "Epoch: 81 | Epoch Time: 0.0m 1.2583777904510498s\n",
      "\tTrain Loss: 0.013 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.186 |  Val. Acc: 48.65%\n",
      "Epoch: 82 | Epoch Time: 0.0m 1.2637135982513428s\n",
      "\tTrain Loss: 0.078 | Train Acc: 95.98%\n",
      "\t Val. Loss: 2.052 |  Val. Acc: 51.63%\n",
      "Epoch: 83 | Epoch Time: 0.0m 1.2567143440246582s\n",
      "\tTrain Loss: 0.029 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.711 |  Val. Acc: 47.87%\n",
      "Epoch: 84 | Epoch Time: 0.0m 1.2595601081848145s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.617 |  Val. Acc: 49.43%\n",
      "Epoch: 85 | Epoch Time: 0.0m 1.2664551734924316s\n",
      "\tTrain Loss: 0.020 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.307 |  Val. Acc: 44.11%\n",
      "Epoch: 86 | Epoch Time: 0.0m 1.2556042671203613s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.625 |  Val. Acc: 50.99%\n",
      "Epoch: 87 | Epoch Time: 0.0m 1.2533724308013916s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.677 |  Val. Acc: 47.23%\n",
      "Epoch: 88 | Epoch Time: 0.0m 1.2570593357086182s\n",
      "\tTrain Loss: 0.009 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.829 |  Val. Acc: 46.45%\n",
      "Epoch: 89 | Epoch Time: 0.0m 1.259704351425171s\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.828 |  Val. Acc: 47.23%\n",
      "Epoch: 90 | Epoch Time: 0.0m 1.257091999053955s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.638 |  Val. Acc: 47.23%\n",
      "Epoch: 91 | Epoch Time: 0.0m 1.253361463546753s\n",
      "\tTrain Loss: 0.017 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.464 |  Val. Acc: 46.45%\n",
      "Epoch: 92 | Epoch Time: 0.0m 1.2606432437896729s\n",
      "\tTrain Loss: 0.013 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.441 |  Val. Acc: 50.99%\n",
      "Epoch: 93 | Epoch Time: 0.0m 1.2646193504333496s\n",
      "\tTrain Loss: 0.011 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.461 |  Val. Acc: 49.43%\n",
      "Epoch: 94 | Epoch Time: 0.0m 1.25956392288208s\n",
      "\tTrain Loss: 0.006 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.468 |  Val. Acc: 48.01%\n",
      "Epoch: 95 | Epoch Time: 0.0m 1.2584619522094727s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.335 |  Val. Acc: 53.34%\n",
      "Epoch: 96 | Epoch Time: 0.0m 1.254948377609253s\n",
      "\tTrain Loss: 0.007 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.330 |  Val. Acc: 51.78%\n",
      "Epoch: 97 | Epoch Time: 0.0m 1.2549974918365479s\n",
      "\tTrain Loss: 0.007 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.383 |  Val. Acc: 45.67%\n",
      "Epoch: 98 | Epoch Time: 0.0m 1.252176284790039s\n",
      "\tTrain Loss: 0.007 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.534 |  Val. Acc: 44.89%\n",
      "Epoch: 99 | Epoch Time: 0.0m 1.260582447052002s\n",
      "\tTrain Loss: 0.006 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.810 |  Val. Acc: 50.21%\n",
      "Epoch: 100 | Epoch Time: 0.0m 1.28090500831604s\n",
      "\tTrain Loss: 0.003 | Train Acc: 100.00%\n",
      "\t Val. Loss: 3.272 |  Val. Acc: 59.30%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7603c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
