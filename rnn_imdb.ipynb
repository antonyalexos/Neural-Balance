{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Define the Fields for processing the dataset\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenizer, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Load the IMDb dataset\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, _ = train_data.split(split_ratio=0.01, random_state=random.seed(SEED))\n",
    "\n",
    "\n",
    "# Split the training data to create a validation set\n",
    "# train_data, valid_data = train_data.split(random_state = torch.manual_seed(SEED))\n",
    "train_data, valid_data = train_data.split(split_ratio=0.7)\n",
    "\n",
    "# Build the vocabulary and load pre-trained word embeddings (GloVe)\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Create iterators for the data\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        # Concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e154b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "# Load the pre-trained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Zero the initial weights of the unknown and padding tokens\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34dd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96732b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8\"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17953756",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0.0m 0.5525639057159424s\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.65%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 42.40%\n",
      "Epoch: 02 | Epoch Time: 0.0m 0.49362874031066895s\n",
      "\tTrain Loss: 0.697 | Train Acc: 55.04%\n",
      "\t Val. Loss: 0.742 |  Val. Acc: 42.40%\n",
      "Epoch: 03 | Epoch Time: 0.0m 0.4852142333984375s\n",
      "\tTrain Loss: 0.685 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.723 |  Val. Acc: 42.40%\n",
      "Epoch: 04 | Epoch Time: 0.0m 0.48531222343444824s\n",
      "\tTrain Loss: 0.681 | Train Acc: 57.13%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 51.63%\n",
      "Epoch: 05 | Epoch Time: 0.0m 0.5240910053253174s\n",
      "\tTrain Loss: 0.678 | Train Acc: 64.03%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 47.87%\n",
      "Epoch: 06 | Epoch Time: 0.0m 0.5589697360992432s\n",
      "\tTrain Loss: 0.663 | Train Acc: 67.11%\n",
      "\t Val. Loss: 0.711 |  Val. Acc: 44.74%\n",
      "Epoch: 07 | Epoch Time: 0.0m 0.5037133693695068s\n",
      "\tTrain Loss: 0.659 | Train Acc: 62.23%\n",
      "\t Val. Loss: 0.731 |  Val. Acc: 43.18%\n",
      "Epoch: 08 | Epoch Time: 0.0m 0.4850637912750244s\n",
      "\tTrain Loss: 0.642 | Train Acc: 60.86%\n",
      "\t Val. Loss: 0.720 |  Val. Acc: 46.31%\n",
      "Epoch: 09 | Epoch Time: 0.0m 0.48534727096557617s\n",
      "\tTrain Loss: 0.601 | Train Acc: 72.74%\n",
      "\t Val. Loss: 0.821 |  Val. Acc: 40.20%\n",
      "Epoch: 10 | Epoch Time: 0.0m 0.4852621555328369s\n",
      "\tTrain Loss: 0.598 | Train Acc: 70.32%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 47.87%\n",
      "Epoch: 11 | Epoch Time: 0.0m 0.49019598960876465s\n",
      "\tTrain Loss: 0.571 | Train Acc: 68.91%\n",
      "\t Val. Loss: 0.810 |  Val. Acc: 47.87%\n",
      "Epoch: 12 | Epoch Time: 0.0m 0.490339994430542s\n",
      "\tTrain Loss: 0.514 | Train Acc: 75.58%\n",
      "\t Val. Loss: 0.866 |  Val. Acc: 47.87%\n",
      "Epoch: 13 | Epoch Time: 0.0m 0.48822927474975586s\n",
      "\tTrain Loss: 0.474 | Train Acc: 78.56%\n",
      "\t Val. Loss: 0.818 |  Val. Acc: 41.76%\n",
      "Epoch: 14 | Epoch Time: 0.0m 0.4853322505950928s\n",
      "\tTrain Loss: 0.470 | Train Acc: 81.16%\n",
      "\t Val. Loss: 0.941 |  Val. Acc: 53.98%\n",
      "Epoch: 15 | Epoch Time: 0.0m 0.49323511123657227s\n",
      "\tTrain Loss: 0.403 | Train Acc: 82.39%\n",
      "\t Val. Loss: 1.161 |  Val. Acc: 60.72%\n",
      "Epoch: 16 | Epoch Time: 0.0m 0.4904344081878662s\n",
      "\tTrain Loss: 0.514 | Train Acc: 76.14%\n",
      "\t Val. Loss: 1.243 |  Val. Acc: 45.53%\n",
      "Epoch: 17 | Epoch Time: 0.0m 0.48811793327331543s\n",
      "\tTrain Loss: 0.324 | Train Acc: 87.41%\n",
      "\t Val. Loss: 0.968 |  Val. Acc: 47.87%\n",
      "Epoch: 18 | Epoch Time: 0.0m 0.4917635917663574s\n",
      "\tTrain Loss: 0.389 | Train Acc: 83.43%\n",
      "\t Val. Loss: 0.987 |  Val. Acc: 47.87%\n",
      "Epoch: 19 | Epoch Time: 0.0m 0.4846012592315674s\n",
      "\tTrain Loss: 0.290 | Train Acc: 90.06%\n",
      "\t Val. Loss: 1.243 |  Val. Acc: 54.76%\n",
      "Epoch: 20 | Epoch Time: 0.0m 0.48419737815856934s\n",
      "\tTrain Loss: 0.327 | Train Acc: 87.79%\n",
      "\t Val. Loss: 1.217 |  Val. Acc: 55.54%\n",
      "Epoch: 21 | Epoch Time: 0.0m 0.5099201202392578s\n",
      "\tTrain Loss: 0.223 | Train Acc: 89.73%\n",
      "\t Val. Loss: 1.188 |  Val. Acc: 50.21%\n",
      "Epoch: 22 | Epoch Time: 0.0m 0.48589110374450684s\n",
      "\tTrain Loss: 0.254 | Train Acc: 88.45%\n",
      "\t Val. Loss: 1.312 |  Val. Acc: 54.76%\n",
      "Epoch: 23 | Epoch Time: 0.0m 0.4849822521209717s\n",
      "\tTrain Loss: 0.173 | Train Acc: 94.41%\n",
      "\t Val. Loss: 1.359 |  Val. Acc: 48.79%\n",
      "Epoch: 24 | Epoch Time: 0.0m 0.488877534866333s\n",
      "\tTrain Loss: 0.167 | Train Acc: 93.89%\n",
      "\t Val. Loss: 1.438 |  Val. Acc: 50.99%\n",
      "Epoch: 25 | Epoch Time: 0.0m 0.4895169734954834s\n",
      "\tTrain Loss: 0.122 | Train Acc: 95.27%\n",
      "\t Val. Loss: 1.502 |  Val. Acc: 56.32%\n",
      "Epoch: 26 | Epoch Time: 0.0m 0.5130698680877686s\n",
      "\tTrain Loss: 0.127 | Train Acc: 94.60%\n",
      "\t Val. Loss: 1.668 |  Val. Acc: 53.98%\n",
      "Epoch: 27 | Epoch Time: 0.0m 0.48967957496643066s\n",
      "\tTrain Loss: 0.083 | Train Acc: 97.54%\n",
      "\t Val. Loss: 1.595 |  Val. Acc: 55.54%\n",
      "Epoch: 28 | Epoch Time: 0.0m 0.4905848503112793s\n",
      "\tTrain Loss: 0.100 | Train Acc: 95.83%\n",
      "\t Val. Loss: 1.908 |  Val. Acc: 54.76%\n",
      "Epoch: 29 | Epoch Time: 0.0m 0.4927043914794922s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.92%\n",
      "\t Val. Loss: 1.789 |  Val. Acc: 52.56%\n",
      "Epoch: 30 | Epoch Time: 0.0m 0.4873957633972168s\n",
      "\tTrain Loss: 0.059 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.763 |  Val. Acc: 48.79%\n",
      "Epoch: 31 | Epoch Time: 0.0m 0.4860203266143799s\n",
      "\tTrain Loss: 0.085 | Train Acc: 97.02%\n",
      "\t Val. Loss: 2.248 |  Val. Acc: 58.52%\n",
      "Epoch: 32 | Epoch Time: 0.0m 0.5029046535491943s\n",
      "\tTrain Loss: 0.084 | Train Acc: 94.94%\n",
      "\t Val. Loss: 1.745 |  Val. Acc: 49.29%\n",
      "Epoch: 33 | Epoch Time: 0.0m 0.4915626049041748s\n",
      "\tTrain Loss: 0.150 | Train Acc: 94.60%\n",
      "\t Val. Loss: 1.392 |  Val. Acc: 59.30%\n",
      "Epoch: 34 | Epoch Time: 0.0m 0.4911651611328125s\n",
      "\tTrain Loss: 0.085 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.321 |  Val. Acc: 59.30%\n",
      "Epoch: 35 | Epoch Time: 0.0m 0.48656606674194336s\n",
      "\tTrain Loss: 0.086 | Train Acc: 97.21%\n",
      "\t Val. Loss: 1.488 |  Val. Acc: 56.32%\n",
      "Epoch: 36 | Epoch Time: 0.0m 0.48662567138671875s\n",
      "\tTrain Loss: 0.097 | Train Acc: 96.17%\n",
      "\t Val. Loss: 1.546 |  Val. Acc: 50.21%\n",
      "Epoch: 37 | Epoch Time: 0.0m 0.48624682426452637s\n",
      "\tTrain Loss: 0.088 | Train Acc: 96.17%\n",
      "\t Val. Loss: 2.241 |  Val. Acc: 47.09%\n",
      "Epoch: 38 | Epoch Time: 0.0m 0.48556089401245117s\n",
      "\tTrain Loss: 0.149 | Train Acc: 92.66%\n",
      "\t Val. Loss: 1.967 |  Val. Acc: 50.21%\n",
      "Epoch: 39 | Epoch Time: 0.0m 0.48584842681884766s\n",
      "\tTrain Loss: 0.115 | Train Acc: 94.08%\n",
      "\t Val. Loss: 1.737 |  Val. Acc: 47.87%\n",
      "Epoch: 40 | Epoch Time: 0.0m 0.48453617095947266s\n",
      "\tTrain Loss: 0.159 | Train Acc: 94.04%\n",
      "\t Val. Loss: 1.504 |  Val. Acc: 45.67%\n",
      "Epoch: 41 | Epoch Time: 0.0m 0.4894237518310547s\n",
      "\tTrain Loss: 0.092 | Train Acc: 95.64%\n",
      "\t Val. Loss: 1.775 |  Val. Acc: 47.87%\n",
      "Epoch: 42 | Epoch Time: 0.0m 0.49091029167175293s\n",
      "\tTrain Loss: 0.065 | Train Acc: 96.69%\n",
      "\t Val. Loss: 2.046 |  Val. Acc: 50.07%\n",
      "Epoch: 43 | Epoch Time: 0.0m 0.48639464378356934s\n",
      "\tTrain Loss: 0.071 | Train Acc: 96.69%\n",
      "\t Val. Loss: 1.924 |  Val. Acc: 53.98%\n",
      "Epoch: 44 | Epoch Time: 0.0m 0.4923574924468994s\n",
      "\tTrain Loss: 0.056 | Train Acc: 98.44%\n",
      "\t Val. Loss: 1.810 |  Val. Acc: 50.99%\n",
      "Epoch: 45 | Epoch Time: 0.0m 0.48844456672668457s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.96%\n",
      "\t Val. Loss: 1.853 |  Val. Acc: 48.01%\n",
      "Epoch: 46 | Epoch Time: 0.0m 0.4873776435852051s\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.44%\n",
      "\t Val. Loss: 1.994 |  Val. Acc: 49.57%\n",
      "Epoch: 47 | Epoch Time: 0.0m 0.49382686614990234s\n",
      "\tTrain Loss: 0.024 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.099 |  Val. Acc: 54.12%\n",
      "Epoch: 48 | Epoch Time: 0.0m 0.4863560199737549s\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.115 |  Val. Acc: 47.23%\n",
      "Epoch: 49 | Epoch Time: 0.0m 0.49327850341796875s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.44%\n",
      "\t Val. Loss: 2.215 |  Val. Acc: 51.78%\n",
      "Epoch: 50 | Epoch Time: 0.0m 0.4890627861022949s\n",
      "\tTrain Loss: 0.021 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.247 |  Val. Acc: 52.56%\n",
      "Epoch: 51 | Epoch Time: 0.0m 0.4875671863555908s\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.317 |  Val. Acc: 51.78%\n",
      "Epoch: 52 | Epoch Time: 0.0m 0.486311674118042s\n",
      "\tTrain Loss: 0.013 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.328 |  Val. Acc: 50.21%\n",
      "Epoch: 53 | Epoch Time: 0.0m 0.4882192611694336s\n",
      "\tTrain Loss: 0.008 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.339 |  Val. Acc: 53.98%\n",
      "Epoch: 54 | Epoch Time: 0.0m 0.4857914447784424s\n",
      "\tTrain Loss: 0.009 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.363 |  Val. Acc: 55.54%\n",
      "Epoch: 55 | Epoch Time: 0.0m 0.48856306076049805s\n",
      "\tTrain Loss: 0.008 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.379 |  Val. Acc: 51.78%\n",
      "Epoch: 56 | Epoch Time: 0.0m 0.48830509185791016s\n",
      "\tTrain Loss: 0.010 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.387 |  Val. Acc: 53.34%\n",
      "Epoch: 57 | Epoch Time: 0.0m 0.4911320209503174s\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.393 |  Val. Acc: 52.56%\n",
      "Epoch: 58 | Epoch Time: 0.0m 0.4860222339630127s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.394 |  Val. Acc: 56.32%\n",
      "Epoch: 59 | Epoch Time: 0.0m 0.48660802841186523s\n",
      "\tTrain Loss: 0.007 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.386 |  Val. Acc: 57.10%\n",
      "Epoch: 60 | Epoch Time: 0.0m 0.48581814765930176s\n",
      "\tTrain Loss: 0.007 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.419 |  Val. Acc: 55.54%\n",
      "Epoch: 61 | Epoch Time: 0.0m 0.48645782470703125s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.371 |  Val. Acc: 57.10%\n",
      "Epoch: 62 | Epoch Time: 0.0m 0.4864528179168701s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.307 |  Val. Acc: 50.21%\n",
      "Epoch: 63 | Epoch Time: 0.0m 0.4887573719024658s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.25%\n",
      "\t Val. Loss: 2.180 |  Val. Acc: 53.34%\n",
      "Epoch: 64 | Epoch Time: 0.0m 0.48803234100341797s\n",
      "\tTrain Loss: 0.008 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.134 |  Val. Acc: 60.87%\n",
      "Epoch: 65 | Epoch Time: 0.0m 0.48610663414001465s\n",
      "\tTrain Loss: 0.008 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.272 |  Val. Acc: 57.10%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Epoch Time: 0.0m 0.49433112144470215s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.348 |  Val. Acc: 55.54%\n",
      "Epoch: 67 | Epoch Time: 0.0m 0.48668789863586426s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.410 |  Val. Acc: 53.98%\n",
      "Epoch: 68 | Epoch Time: 0.0m 0.547368049621582s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.545 |  Val. Acc: 53.20%\n",
      "Epoch: 69 | Epoch Time: 0.0m 0.5132825374603271s\n",
      "\tTrain Loss: 0.005 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.513 |  Val. Acc: 53.98%\n",
      "Epoch: 70 | Epoch Time: 0.0m 0.5004301071166992s\n",
      "\tTrain Loss: 0.006 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.659 |  Val. Acc: 45.67%\n",
      "Epoch: 71 | Epoch Time: 0.0m 0.5102508068084717s\n",
      "\tTrain Loss: 0.005 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.775 |  Val. Acc: 46.45%\n",
      "Epoch: 72 | Epoch Time: 0.0m 0.4878699779510498s\n",
      "\tTrain Loss: 0.005 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.713 |  Val. Acc: 45.67%\n",
      "Epoch: 73 | Epoch Time: 0.0m 0.4869809150695801s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.537 |  Val. Acc: 55.54%\n",
      "Epoch: 74 | Epoch Time: 0.0m 0.486835241317749s\n",
      "\tTrain Loss: 0.005 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.495 |  Val. Acc: 51.78%\n",
      "Epoch: 75 | Epoch Time: 0.0m 0.4813573360443115s\n",
      "\tTrain Loss: 0.007 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.433 |  Val. Acc: 60.09%\n",
      "Epoch: 76 | Epoch Time: 0.0m 0.4898233413696289s\n",
      "\tTrain Loss: 0.023 | Train Acc: 98.96%\n",
      "\t Val. Loss: 2.684 |  Val. Acc: 51.78%\n",
      "Epoch: 77 | Epoch Time: 0.0m 0.4801459312438965s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.606 |  Val. Acc: 54.76%\n",
      "Epoch: 78 | Epoch Time: 0.0m 0.48157787322998047s\n",
      "\tTrain Loss: 0.003 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.614 |  Val. Acc: 50.21%\n",
      "Epoch: 79 | Epoch Time: 0.0m 0.48184800148010254s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.671 |  Val. Acc: 50.21%\n",
      "Epoch: 80 | Epoch Time: 0.0m 0.48128771781921387s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.663 |  Val. Acc: 49.43%\n",
      "Epoch: 81 | Epoch Time: 0.0m 0.47968268394470215s\n",
      "\tTrain Loss: 0.006 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.890 |  Val. Acc: 44.74%\n",
      "Epoch: 82 | Epoch Time: 0.0m 0.48419642448425293s\n",
      "\tTrain Loss: 0.011 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.700 |  Val. Acc: 53.20%\n",
      "Epoch: 83 | Epoch Time: 0.0m 0.48227620124816895s\n",
      "\tTrain Loss: 0.035 | Train Acc: 97.87%\n",
      "\t Val. Loss: 2.988 |  Val. Acc: 41.76%\n",
      "Epoch: 84 | Epoch Time: 0.0m 0.478623628616333s\n",
      "\tTrain Loss: 0.020 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.492 |  Val. Acc: 53.20%\n",
      "Epoch: 85 | Epoch Time: 0.0m 0.4857146739959717s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.254 |  Val. Acc: 48.79%\n",
      "Epoch: 86 | Epoch Time: 0.0m 0.4823570251464844s\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.218 |  Val. Acc: 45.67%\n",
      "Epoch: 87 | Epoch Time: 0.0m 0.47789692878723145s\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.210 |  Val. Acc: 46.45%\n",
      "Epoch: 88 | Epoch Time: 0.0m 0.48075103759765625s\n",
      "\tTrain Loss: 0.010 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.289 |  Val. Acc: 52.56%\n",
      "Epoch: 89 | Epoch Time: 0.0m 0.4791297912597656s\n",
      "\tTrain Loss: 0.006 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.577 |  Val. Acc: 50.99%\n",
      "Epoch: 90 | Epoch Time: 0.0m 0.48002052307128906s\n",
      "\tTrain Loss: 0.004 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.776 |  Val. Acc: 55.54%\n",
      "Epoch: 91 | Epoch Time: 0.0m 0.4806511402130127s\n",
      "\tTrain Loss: 0.006 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.742 |  Val. Acc: 57.10%\n",
      "Epoch: 92 | Epoch Time: 0.0m 0.4790797233581543s\n",
      "\tTrain Loss: 0.004 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.680 |  Val. Acc: 52.56%\n",
      "Epoch: 93 | Epoch Time: 0.0m 0.48684120178222656s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.647 |  Val. Acc: 53.34%\n",
      "Epoch: 94 | Epoch Time: 0.0m 0.4824259281158447s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.629 |  Val. Acc: 53.34%\n",
      "Epoch: 95 | Epoch Time: 0.0m 0.48436570167541504s\n",
      "\tTrain Loss: 0.004 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.680 |  Val. Acc: 54.12%\n",
      "Epoch: 96 | Epoch Time: 0.0m 0.4784085750579834s\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.48%\n",
      "\t Val. Loss: 2.729 |  Val. Acc: 51.78%\n",
      "Epoch: 97 | Epoch Time: 0.0m 0.4783453941345215s\n",
      "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.772 |  Val. Acc: 55.54%\n",
      "Epoch: 98 | Epoch Time: 0.0m 0.47920727729797363s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.29%\n",
      "\t Val. Loss: 2.667 |  Val. Acc: 56.32%\n",
      "Epoch: 99 | Epoch Time: 0.0m 0.4799809455871582s\n",
      "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.337 |  Val. Acc: 53.34%\n",
      "Epoch: 100 | Epoch Time: 0.0m 0.4794588088989258s\n",
      "\tTrain Loss: 0.004 | Train Acc: 100.00%\n",
      "\t Val. Loss: 2.280 |  Val. Acc: 52.56%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7603c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
